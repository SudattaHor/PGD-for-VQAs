# Escaping saddle point in variational quantum algorithms with perturbed gradient descent

Read the report [here](./PGD-for-VQAs-report.pdf).

## Abstract
Variational quantum algorithms (VQAs), which
use classical algorithms to optimize parameter-
ized quantum circuits, are seen as the best hope
to achieve quantum advantage due to their wide
range of applications and compatibility with near-
term quantum computers. However, finding effi-
cient classical optimization strategies for VQAs
can be challenging. For example, the quantum ap-
proximate optimization algorithm (QAOA) has
loss landscapes that are generally non-convex,
and typical gradient descent optimization may
get stuck at bad local optima and saddle points. In
this work, we investigate the use of stochasticity
to escape saddle points. We provide evidence that
additional stochasticity can escape saddle points,
but may also lead to worse solutions

## Instructions for reproducing results
